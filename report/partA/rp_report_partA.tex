\documentclass[a4paper,11pt]{article}%Schriftgröße
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage[german, english]{babel}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage[format=plain,
	justification=RaggedRight,
	singlelinecheck=false,
	font={small},labelsep=space]{caption}
\usepackage{xcolor}	
\usepackage[a4paper]{geometry}
	\geometry{left=3.5cm,right=2.5cm,top=2.4cm,bottom=2cm}%Seitenränder
	\usepackage[onehalfspacing]{setspace}%Zeilenabstand
	\renewcommand{\\}{\vspace*{0.5\baselineskip} \newline}
\renewcommand*\MakeUppercase[1]{#1}	
\usepackage{fancyhdr}
	\pagestyle{fancy}
	\renewcommand{\headrulewidth}{0pt}
	\renewcommand{\footrulewidth}{0pt}
	\fancyhead[R]{\footnotesize{\thepage}}
	\fancyhead[L]{\footnotesize{\leftmark}}
	\fancyfoot{}
\usepackage[colorlinks,
	pdfpagelabels,
	pdfstartview = FitH,
	bookmarks = true,
	bookmarksnumbered = true,
	linkcolor = black,
	urlcolor = black,
	plainpages = false,
	hypertexnames = false,
	citecolor = black] {hyperref}



\usepackage[bottom]{footmisc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage{float}
\usepackage{xurl}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{acronym}
\usepackage{nameref}
\usepackage{setspace}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{color,soul}
\usepackage{multirow}
\usepackage{subfigure}

\usepackage{svg}

\usepackage{stackengine}
\usepackage{trfsigns}
\usepackage{bytefield}
%\usepackage[table]{xcolor}
\usepackage{colortbl}

\newcommand{\colorbitbox}[3]{%
         \rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}%
         \bitbox{#2}{#3}}
\definecolor{lightcyan}{rgb}{0.84,1,1}

% settings
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

% Fußnote linksbündig
\usepackage{scrextend}
\deffootnote[2em]{2em}{1em}{\textsuperscript{\thefootnotemark}\,}

\usepackage{listings}
\usepackage{units}

\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{CStyle}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}

\graphicspath{
    {pictures/}
}

\begin{document}
\pagenumbering{gobble}
\pagenumbering{roman}

\begin{titlepage}
	\centering
	{\scshape\LARGE TH Köln \par}
	\vspace{1cm}
	{\scshape\Large project report\par}
	\vspace{1.5cm}
	{\huge\bfseries Realizing spatial listening tests in Virtual Reality using Unity\par}
	\vspace{2cm}
	{\Large\itshape Alexander Müller \par}
	\vfill
	Supervised by\par
	M.Sc. Melissa Andrea Ramírez Caro \par \&  \par Prof. Dr. Christoph Pörschmann
	\vfill

% Bottom of the page
	{\large \today\par}
\end{titlepage}


\newpage

\tableofcontents
\newpage

\pagenumbering{arabic}


\section{Abstract}
\ac{CAPD} is described by the \ac{ASHA} as a condition, which \dq may lead to or be associated with difficulties in higher order language, learning, and communication functions\dq{} without being caused by actual hearing loss or inabilities \cite{ASHA}. Focusing on the aspect of spatial hearing Cameron and Dillon established the term \ac{SPD} and designed the \ac{LiSN} $\&$ Learn auditory training software to improve binaural processing abilities of affected children \cite{LiSN-A}.
\newline
\newline
Based on this foundation a new training software shall be created as an OpenSource project with \ac{VR} support inside the \textit{Unity} game development engine. Apart from offering a free and easy to use alternative to the follow up product of the original program (\textit{Soundstorm) \footnote{Reference: \url{https://www.soundstorm.app/}}} this project shall also evaluate the possible improvements to the concept using the features of an \ac{VR} application. This includes improved immersion into the auditory environment, the support of real 3D-audio in conjunction with head tracking.

\newpage


% What is spatial hearing (short)
% What are the advantages of spatial hearing?
% Obvious: (orientation, reaction to dangers)
% Not so obvious: directive noise filtering, managing noisy environments
% Who has problems: People with hearing inabilities
% Exception:  spatial processing disorder
% Problems of SPD: noisy classrooms and other social events
% LiSN: explanaition of the original paper/study
% What will be done here
\section{Introduction}
\label{sec:introduction}
Spatial hearing describes the ability, to localize the origin of a given sound event, by using binaural clues of the respective signal. The more obvious advantages of this capability include an aid in orientation and improved possibilities to react to events (like an approaching car), which are currently not within the field of view.
\newline
\newline
But apart from this, spatial hearing also allows to separate different sound sources and helps managing noisy environments. This is especially useful if the incoming audio signals are similar to each other. A popular example for this is the so called \dq cocktail-party-effect\dq{}, which basically described a situation where a listener has to distinguish between a lot of speech signals and focus on a single one in order to be able to maintain a conversation.
\newline
\newline
Even though this scenario does not cause too much problems for the average person, it contains a lot of problems for everyone with hearing inabilities. Unfortunately, in many cases these impairments affect the ability of spatial hearing and therefore reducing or eliminating information that could otherwise be used to separate the different audio sources from each other. Even modern day hearing aids still can't offer the nuances required to precisely determine the location of a given sound source \cite{HA-SRT}.
\newline
\newline
But, apart from physiological issues causing problems with spatial hearing, it has been found that children with normal hearing thresholds still might not be able to correctly interpret binaural clues. This effect is described as \ac{SPD}. Since the actual hearing abilities of the affected children is not impaired, they are - in contrast to persons with typical hearing inabilities - provided with all the information required to localize a given sound source. Based on this knowledge the idea was formed, to treat \ac{SPD} by \textit{training} affected children and therefore teaching them, how to correctly interpret binaural clues.
\newline
\newline
Within the \textit{Development and Evaluation of the \ac{LiSN} $\&$ Learn Auditory Training Software for Deficit-Specific Remediation of Binaural Processing Deficits in Children: Preliminary Findings} \cite{LISN-A} paper by Sharon Cameron and Harvey Dillon it is described how such a training process could look like and already gave some hints \footnote{The original experiments have only been done with a small sample size. Within the conclusion of the paper the requirement of a clinical trial is mentioned, to validate the efficacy of the training process.}.
\newline
\newline
Building upon this approach the training concept described by Cameron and Dillon shall be transferred into a virtual reality application. This shall bring the additional advantage of combining auditory and visual clues and also allows to further extend the process. 


%\newpage
%\section{Introduction B}
%\label{sec:introduction_b}
%Spatial hearing describes the ability to use clues in a given sound event to localize its source. This can be achieved by taking advantage of three main features: the difference in level (1) and phase (2) of a signal between the left and the right ear and the spectral colouring (3) of a known noise, which is dependant on the path of the sound waves around the listeners head and upper body.
%%Spatial hearing describes the ability to use binaural clues to localize the source of a noise. This ability relies upon three main features: the difference in level (1) and phase (2) of a signal between the left and the right ear and the spectral colouring (3) of a known noise, caused by the geometry of the listeners head and upper body.
%\newline
%\newline
%%Apart from being helpful for orientating and quickly identifying reacting to possible dangers (e.g. an approaching car), spatial hearing also allows to better manage noisy environments. This is done by using the information of the directivity 
%Apart from obvious advantages of spatial hearing, like localizing the origin of a given sound (e.g. a nearby car), it also provides the listener an option the filter sounds based on their direction. This is particularly important in noisy environments. This situation is most commonly described in the \dq cocktail-party-effect\dq{} (see \cite{CP} for further details). A listener can use the information of position of a given audio source to reduce noises from other directions within their perception.
%\newline
%\newline
%Since noisy environments with a lot of similar audio signals (in particular speech) are very common within a lot of every day occasions, the advantage of using binaural clues has a great importance. In most cases the lack of this ability is found on people with hearing disabilities. Even if hearing aids have advanced throughout the last decades, they still can't offer their users the same subtle binaural clues as a person with intact hearing abilities.
%\newline
%\newline
%But apart from physiological issues causing problems with spatial hearing it has been found, that children with perfect hearing still might not be able to correctly interpret binaural clues. This effect is described as \ac{SPD}. Since the actual hearing abilities of the affected children is not impaired, they are - in contrast to persons with typical hearing inabilities - provided with all the information required to localize a given sound source. Based on this knowledge the idea was formed, to treat SPD by \textit{training} affected children and therefore teaching them, how to correctly interpret binaural clues.
%\newline
%\newline
%Within the \textit{Development and Evaluation of the LiSN $\&$ Learn Auditory Training Software for Deficit-Specific Remediation of Binaural Processing Deficits in Children: Preliminary Findings} \cite{LISN-A} paper by Sharon Cameron and Harvey Dillon it is described how such a training process could look like and already gave some hints \footnote{The original experiments have only been done with a small sample size. Within the conclusion of the paper the requirement of a clinical trial is mentioned, to validate the efficacy of the training process.}.
%\newline
%\newline
%Building upon this approach the training concept described by Cameron and Dillon shall be transferred into a virtual reality application. This shall bring the additional advantage of combining auditory and visual clues and also allows to further extend the process. 


\section{Motivation}
\label{sec:motivation}
Realizing this listening test as a Unity-based VR application offers several advantages. The first one being the open nature of this project. Since only free-to-use assets have been included, both the compiled application and the source files can be made publicly available. Combined with simple setup required to perform the listening experiments, this will hopefully allow a variety of interested groups to perform these tests to collect further data and offer affected children access to work on the condition.
\newline
\newline
Furthermore the Unity framework in conjunction with the prefabs and scripts, that have been created for this project, it would be relatively easy to extend upon the original training concept. For example the addition of multiple distracters within a given scene could very easily be realized.
\newline
\newline
Apart from the previously mentioned aspects the usage VR peripherals for this project also offers a lot more possiblilites. Some of them will be discussed in \Ref{sec:outlook}. But especially adaptive 3D audio in combination with head tracking would allow for a much more realistic scenario. Also the novelty of the VR headset itself will most likely already spark a lot of interest in the participants in comparison to a regular learning/training game.
 

\section{Fundamentals}
\label{sec:fundamentals}
Before describing the general approach of this project in section \ref{sec:approach}, some underlying principles and theoretical foundations shall be discussed. The contents within this section are not supposed to be used as a general explanation but rather as a brief introduction into the topic.

% How do humans localize audio signals
\subsection{Spatial hearing}
In order to localize the source of a given noise, we can use several clues within the perceived sound event. The most important ones being the differences in time and level between the signal on the right and the left ear (Interaul Time/Level Differences [ITD/ILD]). Additionally there are spectral colouring effects, which are based on the path an audio signal takes from it's source, around the listeners head and upper body into the ears. In comparison to ITD/ILDs this effect can only be applied to well known noises, because the listener has to compare the currently perceived noise with previous times in order to identify the spectral differences and assign them to a general direction.   
\newline
\newline
%Auditive localization is based upon the ability to use binaural clues to localize the source of a noise. This ability relies upon three main features: the difference in level (1) and phase (2) of a signal between the left and the right ear and the spectral colouring (3) of a known noise, caused by the geometry of the listeners head and upper body [REFERENCE].
%The human ability of locating audio sources is based upon two main principles
The human ability of locating audio sources can be divided into two main features. The first being the perception of the time and level differences between an acoustic event on the left and right ear. The different time points at which the signal is perceived at the two ears, translates to a different phase at which the audio wave is registered. On the other side the level differences mainly derive from shadowing caused by the head. Both of these effects are not frequency independent, for lower tones the localization via phase differences works better, while at higher frequencies the level variations offer a better indication.
\newline
\newline
The second way of locating a sound source is based upon the spectral differences that occur through the different paths a sound wave can take around the listeners head and upper body. This effect is strongly influenced by the shape of the outer ear, but also the general geometry of the head makes a difference. Basically the head and the ear can be described as directional filters. This kind of localization is mostly based on experiences. The listener can learn to associate the spectral differences in a known noise to the location from which the noise is originated (e.g. by looking for the sound source).

%The actual localization from these perceived spectral differences is based upon previous experience. This means that well known sounds are generally easier to locate, since the listener has more reference data on how a certain kind of noise sounds coming from different directions. However this type of localization is usually more vulnerable towards mistakes. A well known example for this is the so called \dq Front Back Confusion\dq{}.


% How can spatial information be recorded, recreated and simulated
\subsection{Auralization}
\label{Sec:auralization}
Auralization describes the process of recreating not only a given audio signal but also the spatial information. With correct auralization a recorded scene (e.g. a classical concert in a concert hall) can be rec
\newline
This requires advanced recording processes to capture the spatial information of a scene (e.g. a concert hall)
\newline
Auralization describes the process of adding spatial features to audio signals. The simplest example for this would be the move from mono to stereo audio. However, since these initial steps the options of simulating and re-creating spatial features in digital audio processing have advanced quite a lot. This can be easily seen in many consumer electronic products such as AV-Receivers, Soundbars or Headphones supporting 5.1, 7.1  etc. or in particular \ac{VR} peripherals coming with their own 3D audio frameworks for developers.
\newline
\newline
The basic principle remains more or less the same for all these products. Within a given \textit{listening space} the original auditory environment shall be recreated.


% why is the type of distracter signal important (noise vs speech (same or differnt voice)
\subsection{Hearing perception (energy and similarity)}



% what are the theoretical foundations of learning to localize sound sources by binaural clues?
\subsection{Training abilities}
Within this section it shall be discussed, which improvements are to be expected and how they can be explained.
\newline
\newline
Over the duration of a couple of months children with \ac{SPD} have been able to improve their \ac{SRT} up to 3 dB when being provided with binaural clues within the training game. In comparison the results of the control group haven't changed noticeably. This leaves the participants of the study with \ac{SRT} levels similar to childs with a \ac{CAPD}.
% Give a bit of context to 3 dB?
\newline
\newline
Since the issue of the disorder is cognitive, it is not too surprising that the issue can be combated with training.


\section{Approach}
\label{sec:approach}
Within this section the general design principles and requirements shall be described, before the drafts for how this could be realized will be discussed in section \nameref{sec:concept}.
\newline
\newline
There are three main tasks which shall be used as guidelines for this project:
\begin{enumerate}
\item Re-creating the original software to ensure comparability
\item Only using OpenSource / free license assets to allow for free distribution
\item Create all assets and code segments to be easily adjustable for further extensions
\end{enumerate}

%\subsection{Desing principles}
\paragraph{Comparability} To validate if the new application does actually qualify as a training environment to improve spatial processing abilities, it is sensible to re-create the existing software as close as possible. This would not only allow to compare data from existing research with newly collected data, but also eliminate a lot of effort to check if e.g. the chosen speech stimuli are appropriate for the desired purpose. Additionally if this project does indeed lead to a final application, which could be proven to be a viable alternative, the existing data pools can also be used as reference to evaluate whether extensions (like the addition of head tracking) improves the training effect.


\paragraph{Free distribution} Again assuming the outcome of this project will qualify as a valid training software, the opportunity to be able to make it available as an OpenSource project offers several opportunities. The first one being, that combined with the relatively simple hardware requirements, a free to use software might help improve access to an aid against \ac{SPD} to previously excluded demographics. Additionally a solution, which is more accessible offers the opportunity to gain access to more data, which could be used in further research (e.g. by adding a voluntary option to share progress data within a database). Another aspect would be, that as with any OpenSource project, there is a possibility that other developers or researches use the project as a foundation and extend upon it. This might results in a final product with a widely improved functionality than what would be possible for a single team. However, in any case it is required to only include free to use assets within the implementation to make sure that such a publication won't break any license agreements. So a lot of caution is required when selection third party assets, like sound effects or graphics.


\paragraph{Extensibility} Writing flexible software might be more complicated during the initial development, especially when the scope of the project is rather small. However, to allow the easy addition of new features and simultaneously setting the requirements for a successful OpenSource project, a certain level of flexibility within all parts of the project is necessary. Of course within the given development time frame the extend to that this design principle can be fulfilled is limited, but a lot can be achieved even within the concept phase, if considered and prioritised correctly.



%Before evaluating possible improvements of virtual reality support in listening tests, it has to be made sure that there are no major other influences alternating the results of this test. Therefore the first iteration of the new test environment will be developed as close as possible to the original \ac{LiSN} software. This allows to compare test and training results from this test environment with the data already available from previous experiments. If the new software allows to recreate similar test results, it can be assumed that the unenviable difference don't affect the results in a too drastic manner. Afterwards the data from these initial tests can also be used to validate whether VR exclusive features like head tracking add further benefits to the training process.
%\newline
%\newline
%Since the leading question of this project is the possible improvements of the original training game described in \cite{LiSN}, the development will be strongly orientated on the known parameters.



% subseciton{Development tools}
% Unity
% Oculus SDK
% Visual Studio
% Audacity
\paragraph{Development tools} As already mentioned, the major part of the development will be done in Unity with the addition of Visual Studio as \ac{IDE}. The other major tool is the Oculus \ac{VR} \ac{SDK}, which will be used for audio spatialization and of course rendering the visuals to be properly displayed on the given \ac{VR} peripherals.



\subsection{Unity overview}
\label{sec:unity}
Unity is a development framework, which is mainly used as a engine for video games. However due to the requirements to realism in modern video games, the amount of audio processing included, is already pretty elaborate. In conjunction with the large user base and support by hardware manufacturer (e.g. Oculus), Unity offers a great starting point for this project.


\subsection{Orientation on LiSN project}


\subsection{Audio assets}
\label{sec:audio_assets}
As basis for this project the word lists given in the appendix of the LiSN paper \cite{lins} are used. Since due to the huge variety of options, the only feasible way to construct the sentences is by assembling a sentence through seperate audio files for each word. This however comes with a problem. Simply recording single words and then playing the files one after another strongly alter the speech flow and accentuation, which would normally be present when the sentence is spoken.
\newline
\newline
To handle this problem, the words won't be recorded separately but instead a subset of the possible sentences from each list will be recorded. Within this subset all words from the list are included within the recordings. Afterwards the recorded sentences will be cut into the individual words and then will be used the assemble randomly generated sequences based on these assets. This should work fine, since the structure of all sentences within any list remains the same. So the speech flow and pronunciation between the sentences should not be too larger. GET SOME REFERENCES FOR THIS ASSUMPTION!
\newline
\newline
It also has to be considered that of course the phonetic differences between a given word group will also differ. So in some cases it might be a lot easier for a listener to guess the correct word, since some options of the given selection can be excluded, even if the correct word wasn't fully understood.

\subsection{Visuals}
\label{sec:visuals}
Since this project shall be used to perform research with children as participants the graphical presentation is of great importance. Especially since Virtual Reality (as of today) is in general a quite unfamiliar setting, it is very important to give the user a sense of space within the setting.

\paragraph{Matching audio parameters to visuals} The audio scenario shall be based upon free-field sound propagation. To avoid confusion of the listener the environment presented within the VR application should represent this setting. So a closed room would not be a good option, since we inherently expect reflections and reverberation within such a setting. A better approach would be an open field, where the lack of reflexions feels more natural.

\paragraph{Avatars} Another important part is the graphical representation of the audio sources. Since we have the visual component given through the VR headset, the sound should not simply come from an invisible sources, but have a origin which the user can identify through the graphics. This however includes another challenge. Of course it would be possible to create humanoid avatars with complex animations - including lip syncing - to convey that the object is indeed active and not just a passive talking rock. However this would take a huge amount of effort to pull of. Instead a different path will be used here, which is also pretty common in budget oriented game design (e.g. indie games). Through abstract avatars and simple animations/movements it is possible to convince the user, that the object is alive and active, while at the same time requiring a lot less effort to pull of.
SOURCE...REFERENCES




\subsubsection{Audio Sources}
Within the level both interactive and regular audio sources are implemented. All of them use 3D rendering, so that the preceived sound changes based on the position of the player to the object, emitting the sound. The advantage of adding interactive sound sources is that,...

% Tatsächliche Testumgebung mit Datenerfassung
% 4 Optionen: 0/90 deb, same/different voice
% 4 Listen, zufällige Sätze, 4 Rate Optionen
% Audio parameter (SNR zwischen Talker/Distractor)
% Welche Daten werden erfasst: Totals, Hits, Misses, List, Group (?)
% Wie/Wann wird zwischen Wordgruppen gewechselt?
\subsection{Training-Game}
\label{sec:training_game}


\subsubsection{Audio files}
Target sentences are constructed in a way that the co-articulation in between the sentences of a list is similar enough the allow randomly scrambling up sentences will maintaining a natural sounding result.

\subsubsection{Game loop}
Continuous distracter stories creating a noise with similar characteristics as the target sentences.
\newline
\newline
Both the target sentences and the distracter stories shall be omitted by actual objects within the game world. This association between audio and visual representation shall create further immersion (FIND REFERENCE!).
\newline
\newline
Whenever a target sentence was played, four word options will be displayed. (IMAGES OR WORDS?). When the correct result is picked, the selected option turns green and a celebratory sound is played. Otherwise if an incorrect option is selected, it is marked red and a failure sound is played. Alternatively an  \dq uncertain\dq{} option is presented.
\newline
\newline
SNR: For every correct guess, the SNR is decreased by 2.5 dB. For every incorrect guess, SNR is increased by 1.5 dB. At uncertain at first the same sentence is repeated with 1.5 dB higher SNR. At a second consecutive \textit{uncertain} a new sentence will be played, with again increased SNR.


\section{Concept}
\label{sec:concept}
While section \ref{sec:approach} only described the general design principles and foundations of the application, this section shall be used to discuss how these requirements  actually be fulfilled and which parts of the project might cause issues.
\newline
\newline
The biggest part will be achieving flexibility in the projects structure, to allow other users to adapt the application to their requirements.


\subsection{Training Game}


\subsubsection{Audio Sources}
Inside the training game there will be two possible setup option for the audio sources. In \textbf{Setup A} the \textit{target} audio source will be placed directly in front of the \textit{main camera}, while the \textit{distracter} will be moved 90 degrees to the right side \footnote{The selection between right and left side is arbitrary. Interchanging both options during or between sessions would also be an option.}. In \textbf{Setup B} both audio sources will be placed directly in front of the \textit{main camera} and therefore no spatial information can be used to differentiate both audio streams. In both options the distance of the sources to the \textit{main camera} hjas to be the same.
%\vspace{5mm}
\begin{figure}[h!]
	\hspace{0.1\textwidth}
    \subfigure[Setup A]{\includegraphics[width=0.4\textwidth]{trainingSetupPageA.png}}
    %\hspace{0.2\textwidth}
    \subfigure[Setup B]{\includegraphics[width=0.4\textwidth]{trainingSetupPageB.png}}
    %\hspace{0.1\textwidth}
\caption{Training Game Setup}
\label{fig:setups}
\vspace{3mm}
\end{figure}
\newline
%\vspace{8cm}
\newline

\subsubsection{Word databases}
Even though the initial scope of the application shall only feature a single word list from the original paper (Appendix A \cite{LiSN-A}), the implementation should be flexible enough to support different word databases. This is particularly important when considering the option to port the application to different languages. To further elaborate on how this should be done, List 1 from the original paper will be used as an example.  Within table \ref{tab:list1} all highlighted words are can be used as an \textit{option} to question the user.
\begin{table}[h!]
\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
& Subject & Verb & Count & Adjective & Objects \\
\hline
The & \textbf{baby} & bought & \textbf{two} & big & \textbf{apples} \\
\cline{2-6}
 & \textbf{boy} & carried & \textbf{three} & blue & \textbf{bottles} \\
\cline{2-6}
 & \textbf{clown} & cleaned & \textbf{four} & borken & \textbf{cars} \\
\cline{2-6}
 & \textbf{doctor} & drew & \textbf{five} & green & \textbf{chairs} \\
\cline{2-6}
 & \textbf{girl} & dropped & \textbf{six} & little & \textbf{crayons} \\
\cline{2-6}
 & \textbf{lady} & had & \textbf{seven} & old & \textbf{cups} \\
\cline{2-6}
 & \textbf{man} & liked & \textbf{eight} & orange & \textbf{shoes} \\
\cline{2-6}
 & \textbf{nurse} & saw & \textbf{nine} & red & \textbf{spoons} \\
\cline{2-6}
 & \textbf{teacher} & watched & \textbf{ten} & yellow & \textbf{trucks} \\
\hline
\end{tabular}
\caption{LiSN - List 1}
\label{tab:list1}
\end{table}
\vspace{5mm}
When observing this table three parameters can be determined:
\begin{itemize}
\item Sentences length: \textbf{6 words}
\item Possibilities for each group: \textbf{9 options}
\item Number of 'selectable' groups: \textbf{3 selectable groups}
\end{itemize}
\vspace{5mm}
Of course this could be extended even further, like adding sentences of variable length or alternating the 'selectable groups'. But as with many of these decisions, offering too many options might lead to problems (e.g. the generated sentences won't be as comparable if they are allow to differ in length).
\newline
\newline
All three of these parameters have to be considered when creating a dynamic framework, which shall be able to support different word lists. Of course it 
\newline
Additionally this example also provides and interesting anomaly. All \textit{options} within a given \textit{group} are sorted alphabetically, except the \textbf{Count} - words. It is important to not rely on any order in which the list might be created, or any other parameter that depends on the actual content of a given word list.
\newline
\newline
Issues like co-articulation effects have to be considered by the creator of a given group.


\subsection{Asset management}
\label{sec:asset_management}
Moving on from the requirement of establishing a framework, which supports variable \textit{word database} formats the topic of how assets can be added or changed within the application has to be considered.
\newline
\newline
Unity offers multiple ways to handle this topic. The straight forward approach would be to assign the individual asset files through drag and drop within the inspector. Even though this would in principle fulfil the requirement of changing/adding assets, it's not ideal when many assets shall be changed at once (consider List 1 (table \ref{tab:list1}, where already 9 x 5 audio files for the individual words would have to be added manually). 
\newline
\newline
Instead the \textit{Resource} system shall be used to handle this group of assets. This offers the opportunity to load assets straight from the file system into the application. However this solution also has some drawbacks. At first the path of the assets within the file system has to be considered. Once option would be to enforce a main path with naming policies. A path could then look like this: \textit{Resources/Audio/WordList<\textbf{number}>/Group<\textbf{number}>}.


\subsection{Progress tracking}
In order for this application to be useful, an option has to be added to track the progress of the user over the course of multiple training sessions. At first it has to be defined, which information has to be tracked in order receive all required data to not only monitor possible improvements of a single user but also to compare the results of multiple participants against each other within the context of a study.
\newline
\newline
The most obvious parameter to be tracked would be the \ac{SRT}. In order to recognize a progress this has to be stored for each training session. It would of course also be possible to not only keep the average \ac{SRT} value of each sessions, but also the \ac{SNR}s of each individual sentences combined with the information if the guess was \textit{correct}, \textit{incorrect} or \textit{unsure}. However, even though both the target and distracter audio assets have been normalized there might still be some fluctuations within the perceived sound levels of both sources, which can't be tracked or even noise from outside of the application. Therefore this data would come with several uncertainties and is also not required if the data evaluation shall be done in comparison to the original paper.
\newline
\newline
Another important part would be to allow for differentiation between regular participants of a study and a control group. Therefore a \textbf{group} parameter should also be added.


\paragraph{User System} Another part which has to be considered, is the option that more than one person might use the same system to do training sessions. To make sure that the progress can be tracked individually, a user system shall be established. Within this system access to the \textit{training game} shall be restricted via a typical login screen requiring the correct \textbf{username} and \textbf{password} to be entered. Adding new users should also be possible from the login screen, where \textbf{username} and \textbf{password} can freely be set (as long as the username isn't already in use). Within the 'user creation' progress the already mentioned differentiation between regular and control group could also be added and be linked to the user account.


\paragraph{Progress export} Next to tracking the progress data for each user, it should also be made accessible from outside of the application in a format that allows easy evaluation with external tools. For this purpose all user data shall be stored within a \textit{.json} file. This allows to hold multiple types of data from \textit{strings} for the username to arrays of \textit{floats} for the \ac{SRT} values and easily readable as well as widely used in many different applications.


\paragraph{In-app visualization} Finally to application shall give any user an option to keep track of their progress. This will be realized through a separate screen, which features a simple graph plotting the \ac{SRT} values of the amount of traning sessions done as well as additional space to show e.g. the current average \ac{SRT} over all sessions.


\subsection{User interface}
\label{sec:ui-concept}
Based on the previously described parts of the application, a general setup for the required \ac{UI} options can be defined. At first it is important, that all \ac{UI} elements can easily be selected while using \ac{VR} peripherals. This could for example be achieved by setting an appropriate minimal size for all text that displayed. Since a lot of this 
\newline
\newline
Apart from this general consideration, a brief overview of all major \ac{UI} elements shall be given


\paragraph{Main Menu} The \textit{Main Menu} only requires button to access different screens (e.g. \textit{Training Game} or \textit{Progress}).

\paragraph{Login} This requires of course text fields which can be used to type down both the username and password. Additionally a \textit{Submit} and a \textit{Create new user} button are necessary.

\paragraph{User Creation} This can basically be the same as the \textit{Login Screen}, with the addition that the user has to select a group (control or regular) to be associated with the account.

\paragraph{Progress} As already described, the \textit{Progress} screen shall display a combination of different information. As interactive object a \textit{Return} button has to be added, to allow the user to return to the \textit{Main Menu}.

\vspace{5mm}
\begin{figure}[h!]
\centering
\includegraphics[width=0.7\textwidth]{screens2.png}
caption{Screens}
\label{fig:screens}
\vspace{3mm}
\end{figure}



% Progress
% TG: Voice Selection
% TG: Word Selection



\section{Conclusion}
\label{sec:conclusion}
Within part A of this research project, a concept as well as the main set of requirements for recreating the \ac{LiSN} software has been presented. Of course as within every software development process it is to be expected that some parts of the final product will derive from the initial concept, due to either new insights into the subject matter or unforeseen difficulties during the implementation.


%Within the scope of this project a simple binaural recreation plug-in has been implemented using the Juce framework and the FFTW library. Unfortunately the focus of this project has been moved heavily onto software development and typical coding issues, instead of digging deeper into the actual subject of audio auralization. But nonetheless the current state of the plug-in might be a good starting point for further projects, at cutting some of the very time consuming setup procedures. Of course this would require to encapsulate the currently implemented processing further into individual functions, or possibly even better an additional class. With a bit added abstraction, a generic processing chain could be implemented (similar to the one already implemented by Juce), which would allow to add and move individual auralization processes much more easily. Another working point would be to amp up the current convolution, to also feature e.g. elevation. This not too much would have to be changed in the elemental parts of the source code, this should be a rather simple addition.



\newpage
\section*{Abbreviations}
\vspace{5mm}
\begin{acronym}[SEVENLTH]
 \acro{API}{Application Programming Interface}
 \acro{ASHA}{American Speech-Language-Hearing Association}
 \acro{CAPD}{Central Auditory Processing Disorder}
 \acro{DAW}{Digital Audio Workstation}
 \acro{GUI}{Graphical User Interface}
 \acro{HRTF}{Head Related Transfer Function}
 \acro{IDE}{Integrated Development Environment}
 \acro{LiSN}{Listening in Spatialized Noise}
 \acro{SDK}{Software Development Kit}
 \acro{SNR}{Signal to Noise Ratio}
 \acro{SPD}{Spatial Processing Disorder}
 \acro{SRT}{Speech Reception Threshold}
 \acro{UI}{User Interface}
 \acro{VR}{Virtual Reality}
\end{acronym}

\newpage
\listoffigures
\addcontentsline{toc}{section}{List of Figures}
%\newpage
\renewcommand\refname{Sources}
\addcontentsline{toc}{section}{Sources}
\begin{thebibliography}{14}
%\bibitem{OCU_SPA}{Oculus Spatializer: \url{https://developer.oculus.com/documentation/unity/audio-osp-unity-spatialize/}}
\bibitem{LiSN-A}{\textit{Development and Evaluation of the LiSN $\&$ Learn Auditory Training Software for Deficit-Specific Remediation of Binaural Processing Deficits in Children: Preliminary Findings}, Sharon Camerion, Harvey Dillon, Date: 2011}
\bibitem{LiSN-B}{\textit{Correlating performance on the Listening in Spatialized Noise – Sentences Test (LiSN-S) with the Listening in Spatialized Noise – Universal Test (LiSN-U)}, Kiri Mealingsa, Sharon Camerona and Harvey Dillon, Published: April 2020, Date: 2019}
\bibitem{CP}{The cocktail-party problem revisited: early processing and selection of multi-talker speech, Adelbert W. Bronkhosrt, Date: April 2015}
\bibitem{HA-SRT}{\textit{Listening through hearing aids affects spatial perception and speech intelligibility in normal-hearing listeners}, Jens Cubick, Jörg M Buchholz, Virginia Best, Mathieu Lavandier, Torsten Dau, Date: 2016, \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6246072/}}
\bibitem{ASHA}{\textit{Central Auditory Processing Disorder}, \acf{ASHA}, \url{https://www.asha.org/Practice-Portal/Clinical-Topics/Central-Auditory-Processing-Disorder/}}
\end{thebibliography}

\end{document}